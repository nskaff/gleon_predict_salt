
Space and NLA data. 
author: "Shannon LaDeau"
date: "October 2018"

---

```{r}
rm(list = ls())  #remove anything stored in workspace from previous exercises

#install.packages("tidyverse")
#library(readxl)
library(readr)
library(rjags)
library(tidyverse)
library(gridExtra)
```


Data used are from the US National Lakes Assessment: https://www.epa.gov/national-aquatic-resource-surveys/nla
The focal response variable here is chloride

```{r setup}
dat = read_csv("data/GLEON NLA data combined 2007 and 2012_first_visit.csv") # note: read_csv needs readr package installed

##splitting data into training and testing set, with 80% of data in the training set
dat$id <- 1:nrow(dat)
train <- dat %>% dplyr::sample_frac(.8)
test  <- dplyr::anti_join(dat, train, by = 'id')

y = train$CHLORIDE_MGL  #focal response variable

region= train$OMER_L3_ECO  #ecological region indicator

depth = train$DEPTH_M.mean - mean(train$DEPTH_M.mean,na.rm=T) #centered predictor


temp = train$TEMP_C.mean - mean(train$TEMP_C.mean,na.rm=T)#centered predictor


urban = 0.01 * (train$PCT_DEVELOPED_BSN- mean(train$PCT_DEVELOPED_BSN,na.rm=T)) #centered predictor - as proportion

ag = 0.01 * (train$PCT_AGRIC_BSN- mean(train$PCT_AGRIC_BSN,na.rm=T)) #centered predictor - as proportion

forest = 0.01 * (train$PCT_FOREST_BSN- mean(train$PCT_FOREST_BSN, na.rm=T)) #centered predictor - as proportion

lat=train$LAT_DD - mean(train$LAT_DD,na.rm=T)  #centered predictor

resT = train$Residence_Time - mean(train$Residence_Time, na.rm=T)

####setting up testing parameters
y.test = test$CHLORIDE_MGL  #focal response variable

region.test= test$OMER_L3_ECO  #ecological region indicator

depth.test = test$DEPTH_M.mean - mean(test$DEPTH_M.mean,na.rm=T) #centered predictor


temp.test = test$TEMP_C.mean - mean(test$TEMP_C.mean,na.rm=T)#centered predictor


urban.test = 0.01 * (test$PCT_DEVELOPED_BSN- mean(test$PCT_DEVELOPED_BSN,na.rm=T)) #centered predictor - as proportion

ag.test = 0.01 * (test$PCT_AGRIC_BSN- mean(test$PCT_AGRIC_BSN,na.rm=T)) #centered predictor - as proportion

forest.test = 0.01 * (test$PCT_FOREST_BSN- mean(test$PCT_FOREST_BSN, na.rm=T)) #centered predictor - as proportion

lat.test=test$LAT_DD - mean(test$LAT_DD,na.rm=T)  #centered predictor

resT.test = test$Residence_Time - mean(test$Residence_Time, na.rm=T)
```




If you're comfortable running the code then YOU SHOULD be able to scroll down to the section titled "add Multiple Predictor Variables" and skip the sequential models that add RE and single variable.

Confirm convergence and define burn-in. ONCE you have a model you like you should do the steps below to make sure model is converged AND that you only use samples from posterior afer convergence.

##ignore this code for now
```{r}
gelman.diag(jags.out)
GBR <- gelman.plot(jags.out)

```
##ignore this code for now
```{r}
## determine the first iteration after convergence
burnin <- GBR$last.iter[tail(which(GBR$shrink[,,2] > 1.1),1)+1]## check for no burn-in case
if(length(burnin) == 0) burnin = 1
## remove burn-in
jags.burn <- window(jags.out,start=burnin)
## check diagnostics post burn-in
gelman.diag(jags.burn)
plot(jags.burn)
```

Model with multiple predictor variables 

```{r}
NormMean.Cov.RE <- "
model {
  beta0 ~ dnorm(0,tau.beta0) # priors  
  beta1~ dnorm(0,tau.beta1)
  beta2~ dnorm(0,tau.beta2)
  beta3~ dnorm(0,tau.beta3)

  tau.obs ~ dgamma(a_obs,r_obs) #prior precision, data model

  for(i in 1:N){
    mu[i] <- beta0 + beta1 * urban[i]+ beta2 *ag[i] + beta3*forest[i] + reg[region[i]] # process model 
    y[i] ~ dnorm(mu[i],tau.obs) #data model
    urban[i]~dnorm(0,50)  ##prior based on existing summary
    ag[i]~dnorm(0,50)
    forest[i]~dnorm(0,50)

  }
 for(r in 1:maxR){  
    reg[r] ~ dnorm(0,tau_reg)
 }
tau_reg  ~ dgamma(0.01,0.01) ##prior precision,  random effect
}
"
```

Define the data

```{r}

data = list(N=length(y),y = log(y), region = region, maxR=max(region),  urban=urban, ag=ag, forest=forest,
            tau.beta0=1/0.01, tau.beta1=1/0.01, tau.beta2=1/0.01, tau.beta3=1/0.01,
          a_obs = 0.01, r_obs= 0.01 ) ## prior mean precision 

nchain = 3  # num chains for MCMC

##Set-up initial values - okay to use data to identify good starting points
inits <- list()
for(i in 1:nchain){
  y.samp = sample(log(y),length(y),replace=TRUE)  #resamples orig data 
  inits[[i]] <- list(tau.obs=1/var((y.samp)), beta0=mean(y.samp), tau_reg=1.2)
}

```

##Run the model
```{r}
j.model   <- jags.model (file = textConnection(NormMean.Cov.RE),
                             data = data,
                             inits = inits,
                             n.chains = 3)

```

```{r}
jags.out   <- coda.samples (model = j.model,
                              
                            variable.names = c("beta0","beta1", "beta2", "beta3", "tau.obs","tau_reg"), 
                                n.iter = 5000)

plot(jags.out)

out<-as.matrix(jags.out)
summary(out)

#DIC is easy way to look at model fit (generally look for decrease by 10 or more)
dic.samples(j.model,1000)  ##Deviance Information Criterion

```

Getting predicted values and calculating training and testing R2 and rmse. This should be double checked..not totally sure of myself here

```{r}
jags.out   <- jags.samples (model = j.model,variable.names = c("beta0","beta1", "beta2", "beta3", "reg","mu", "y"),  n.iter = 5000)

#taking the mean of the posterior for each parameter estimated
posterior_means <- lapply(jags.out, apply, 1, "mean")


###calculating training R2###
rss <- sum((posterior_means$mu- posterior_means$y) ^ 2, na.rm=T)  ## residual sum of squares
tss<- sum((posterior_means$y - mean(posterior_means$y, na.rm=T)) ^ 2, na.rm=T)  ## total sum of squares
rsq<- 1 - rss/tss


###calculating training rmse###
RMSE = function(pred, obs){
  sqrt(mean((pred - obs)^2))
}
rmse<-RMSE(posterior_means$mu,posterior_means$y)

###calculating training back transformed rmse###
RMSE_back = function(pred, obs){
  sqrt(mean((exp(pred) - exp(obs))^2))
}
rmse_back<-RMSE_back(posterior_means$mu,posterior_means$y)

####calculating training back transformed median absolute error, which doesn't overweight huge errors on outliers like rmse does
#https://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error
MAE_back <- function(pred , obs)
{
    median(abs(exp(pred) - exp(obs)))
}

mae_back<-MAE_back(posterior_means$mu,posterior_means$y)

#organizing the posterior parameter means
B <- as.matrix(unlist(posterior_means[c("beta0","beta1", "beta2", "beta3")]))

#organizing the random intercepts for each region
R.intercept<- as.matrix(unlist(posterior_means[c("reg")]))

###predicting from test set
y_hat_conditional<- B[1,] + B[2,] * urban.test+ B[3,]*ag.test + B[4,]*forest.test + R.intercept[region.test]

#calculating predictive R
rss_cond <- sum((y_hat_conditional - log(y.test)) ^ 2, na.rm=T)  ## residual sum of squares
tss_cond <- sum((log(y.test) - mean(log(y.test), na.rm=T)) ^ 2, na.rm=T)  ## total sum of squares
rsq_conditional <- 1 - rss_cond/tss_cond


###calculating testing rmse###
RMSE = function(pred, obs){
  sqrt(mean((pred - obs)^2, na.rm=T))
}
rmse_test<-RMSE(y_hat_conditional,log(y.test))


###calculating testing rmse back transformed###
RMSE_back = function(pred, obs){
  sqrt(mean((exp(pred) - exp(obs))^2, na.rm=T))
}
rmse_back_test<-RMSE_back(y_hat_conditional,log(y.test))


####calculating testing back transformed median absolute error, which doesn't overweight huge errors on outliers like rmse does
#https://scikit-learn.org/stable/modules/model_evaluation.html#median-absolute-error
MAE_back <- function(pred , obs)
{
    median(abs(exp(pred) - exp(obs)), na.rm=T)
}

mae_back_test<-MAE_back(y_hat_conditional,log(y.test))



##creating a table with training and testing performance
fit.tbl<-data.frame(Fit=c( "RMSE","RMSE^e","MAE^e" ,"R^2"), Train=c(rmse,rmse_back,mae_back,rsq), Test=c(rmse_test,rmse_back_test,mae_back_test,rsq_conditional))

fit.tbl                 
grid.table(fit.tbl)

########not sure how to calculate a marginal R square (not including the intercept) in a bayesian framework...help..

#another way of calculating predictive R2, essentially the same result
#cor(y_hat_conditional,log(y.test), use="pairwise.complete.obs")^2



```




If you want to check out the estimates for the unknown predictor or response variables, it is easiest to run model again and just save those...

```{r}

jags.out2   <- coda.samples (model = j.model,
                            variable.names = c("urban"),
                                n.iter = 1000)

out<-as.matrix(jags.out2)


hist(out[,2070])  ###look at histogram of first missing depth value - row 2070

 dep.mean = rep(0,length(resT))

for (i in 1:length(depth)){dep.mean[i] = mean(out[,i])}
hist(dep.mean[2070])
barplot(dep.mean)
```

